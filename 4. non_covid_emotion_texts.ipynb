{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45571f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "lesoir.be\n",
      "===================\n",
      "Extraction OK\n",
      "Preprocessing OK\n",
      "Sentiment Analysis OK\n",
      "\n",
      "===================\n",
      "lalibre.be\n",
      "===================\n",
      "Extraction OK\n",
      "Preprocessing OK\n",
      "Sentiment Analysis OK\n",
      "\n",
      "===================\n",
      "rtbf.be\n",
      "===================\n",
      "Extraction OK\n",
      "Preprocessing OK\n",
      "Sentiment Analysis OK\n",
      "\n",
      "===================\n",
      "rtlinfo.be\n",
      "===================\n",
      "Extraction OK\n",
      "Preprocessing OK\n",
      "Sentiment Analysis OK\n",
      "\n",
      "===================\n",
      "dhnet.be\n",
      "===================\n",
      "Extraction OK\n",
      "Preprocessing OK\n",
      "Sentiment Analysis OK\n",
      "\n",
      "===================\n",
      "sudinfo.be\n",
      "===================\n",
      "Extraction OK\n",
      "Preprocessing OK\n",
      "Sentiment Analysis OK\n",
      "\n",
      "===================\n",
      "lavenir.net\n",
      "===================\n",
      "Extraction OK\n",
      "Preprocessing OK\n",
      "Sentiment Analysis OK\n",
      "\n",
      "===================\n",
      "lecho.be\n",
      "===================\n",
      "Extraction OK\n",
      "Preprocessing OK\n",
      "Sentiment Analysis OK\n",
      "\n",
      "===================\n",
      "levif.be\n",
      "===================\n",
      "Extraction OK\n",
      "Preprocessing OK\n",
      "Sentiment Analysis OK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "nlp = spacy.load('fr_core_news_lg')\n",
    "\n",
    "relevant_sources = ['lesoir.be', 'lalibre.be', 'rtbf.be', 'rtlinfo.be', 'dhnet.be', 'sudinfo.be', 'lavenir.net', 'lecho.be', 'levif.be']\n",
    "\n",
    "\n",
    "def extract_texts(source_path, tmp_path, start_date=None, end_date=None):\n",
    "    if not os.path.exists(tmp_path):\n",
    "        os.makedirs(tmp_path)\n",
    "\n",
    "    for filename in os.listdir(source_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            date_str = filename.split('_')[1].split('.')[0]\n",
    "            file_date = datetime.strptime(date_str, '%Y%m%d').date()\n",
    "            if start_date and file_date < start_date:\n",
    "                continue\n",
    "            if end_date and file_date >= end_date:\n",
    "                continue\n",
    "            \n",
    "            output_filename = os.path.splitext(filename)[0]\n",
    "            tmp_file_path = os.path.join(tmp_path, f\"texts_{output_filename}.txt\")\n",
    "\n",
    "            if os.path.exists(tmp_file_path):\n",
    "                continue\n",
    "\n",
    "            json_file_path = os.path.join(source_path, filename)\n",
    "            if os.path.exists(json_file_path) and os.path.getsize(json_file_path) > 0:\n",
    "                with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    for line in file:\n",
    "                        json_data = json.loads(line)\n",
    "                        title = json_data.get(\"title\")\n",
    "                        text = json_data.get(\"text\")\n",
    "                        if title and text:\n",
    "                            title = title.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \").replace(\"\\f\", \" \").replace(\"\\v\", \" \")\n",
    "                            text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \").replace(\"\\f\", \" \").replace(\"\\v\", \" \")\n",
    "\n",
    "                            with open(tmp_file_path, \"a\", encoding=\"utf-8\") as tmp_file:\n",
    "                                tmp_file.write(title + \". \" + text + \"\\n\")\n",
    "                        elif title:\n",
    "                            with open(tmp_file_path, \"a\", encoding=\"utf-8\") as tmp_file:\n",
    "                                tmp_file.write(title + \".\\n\")\n",
    "                        else:\n",
    "                            with open(tmp_file_path, \"a\", encoding=\"utf-8\") as tmp_file:\n",
    "                                tmp_file.write(\"\\n\")\n",
    "            else:\n",
    "                with open(tmp_file_path, \"w\", encoding=\"utf-8\") as tmp_file:\n",
    "                    tmp_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_punct]\n",
    "    return tokens\n",
    "\n",
    "def preprocess_texts(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file_path = os.path.join(input_folder, filename)\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            if os.path.exists(output_file_path):\n",
    "                continue\n",
    "\n",
    "            with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, \\\n",
    "                    open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "                if os.path.exists(input_file_path) and os.path.getsize(input_file_path) > 0:\n",
    "                    for line in input_file:\n",
    "                        text = line.strip()\n",
    "                        tokens = tokenize_and_lemmatize(text)\n",
    "                        processed_text = \" \".join(tokens)\n",
    "                        if processed_text:\n",
    "                            output_file.write(processed_text + \"\\n\")\n",
    "                        else:\n",
    "                            output_file.write(\"\\n\")\n",
    "                else:\n",
    "                    output_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def load_dictionary(emotion_dict_path):\n",
    "    emotion_dict = {}\n",
    "    regex_patterns = {}\n",
    "\n",
    "    with open(emotion_dict_path, newline='', encoding='utf-8') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile, delimiter=';')\n",
    "        for row in csv_reader:\n",
    "            word = row['word']\n",
    "            polarity = np.array([int(row['positive']), int(row['negative'])])\n",
    "            emotions = np.array([int(row['joy']), int(row['fear']), int(row['sadness']),\n",
    "                                 int(row['anger']), int(row['surprise']), int(row['disgust'])])\n",
    "\n",
    "            # Compiler l'expression régulière pour trouver des correspondances avec des caractères blancs autour\n",
    "            regex_patterns[word] = re.compile(r'\\b' + re.escape(word) + r'\\b', re.IGNORECASE)\n",
    "\n",
    "            if word in emotion_dict:\n",
    "                # Fusionner les vecteurs d'émotions des doublons en prenant la valeur maximale\n",
    "                emotion_dict[word]['polarity'] = np.maximum(emotion_dict[word]['polarity'], polarity)\n",
    "                emotion_dict[word]['emotions'] = np.maximum(emotion_dict[word]['emotions'], emotions)\n",
    "            else:\n",
    "                emotion_dict[word] = {'polarity': polarity, 'emotions': emotions}\n",
    "\n",
    "    return emotion_dict, regex_patterns\n",
    "\n",
    "\n",
    "class Feel:\n",
    "    def __init__(self, text, emotion_dict, regex_patterns):\n",
    "        self.text = text.lower()\n",
    "        self.polarity = np.zeros(2)\n",
    "        self.emotion = np.zeros(6)\n",
    "        self.occurrences = {}\n",
    "        self.emotion_dict = emotion_dict\n",
    "        self.regex_patterns = regex_patterns\n",
    "        self.num_words = len(text.split())\n",
    "\n",
    "    def get_sentiment(self):\n",
    "        text = self.text\n",
    "        num_words = self.num_words\n",
    "        emotions_mapping = {} \n",
    "        \n",
    "        for emotion_token, data in self.emotion_dict.items():\n",
    "            polarity_value, emotion_value = data['polarity'], data['emotions']\n",
    "            emotions_mapping[emotion_token] = {\n",
    "                'polarity': polarity_value,\n",
    "                'emotions': emotion_value\n",
    "            }\n",
    "            \n",
    "        for emotion_token, data in emotions_mapping.items():\n",
    "            emotion_data = emotions_mapping.get(emotion_token)\n",
    "\n",
    "            if emotion_data is not None:\n",
    "                polarity_value = emotion_data['polarity']\n",
    "                emotion_value = emotion_data['emotions']\n",
    "\n",
    "                regex = self.regex_patterns[emotion_token]\n",
    "                matches = regex.findall(text)\n",
    "                count = len(matches)\n",
    "\n",
    "                if count > 0:\n",
    "                    self.occurrences[emotion_token] = count  \n",
    "\n",
    "                    if not np.array_equal(polarity_value, [0, 0]):\n",
    "                        self.polarity += count * polarity_value\n",
    "                    if not np.array_equal(emotion_value, [0, 0, 0, 0, 0, 0]):\n",
    "                        self.emotion += count * emotion_value\n",
    "\n",
    "        if self.num_words != 0:\n",
    "            self.polarity /= self.num_words\n",
    "            self.emotion /= self.num_words\n",
    "\n",
    "        return self.polarity, self.emotion, self.occurrences\n",
    "    \n",
    "\n",
    "def process_texts(input_folder, output_folder, emotion_dict, regex_patterns):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file_path = os.path.join(input_folder, filename)\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            if os.path.exists(output_file_path):\n",
    "                continue\n",
    "\n",
    "            with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, \\\n",
    "                    open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "\n",
    "                if os.path.exists(input_file_path) and os.path.getsize(input_file_path) > 0:\n",
    "\n",
    "                    for line in input_file:\n",
    "                        if line:\n",
    "                            text = line.strip()\n",
    "                            feel = Feel(text, emotion_dict, regex_patterns) \n",
    "                            feel.text = text\n",
    "                            polarities, emotions, occurrences = feel.get_sentiment()\n",
    "                            \n",
    "                            output_data = {\n",
    "                                'polarity': {\n",
    "                                    'positive': polarities[0],\n",
    "                                    'negative': polarities[1]\n",
    "                                },\n",
    "                                'emotions': {\n",
    "                                    'joy': emotions[0],\n",
    "                                    'fear': emotions[1],\n",
    "                                    'sadness': emotions[2],\n",
    "                                    'anger': emotions[3],\n",
    "                                    'surprise': emotions[4],\n",
    "                                    'disgust': emotions[5]\n",
    "                                },\n",
    "                                'occurrences': occurrences\n",
    "                            }\n",
    "\n",
    "                            if occurrences:\n",
    "                                output_file.write(json.dumps(output_data, ensure_ascii=False) + \"\\n\")\n",
    "                            else:\n",
    "                                output_file.write(\"\\n\")\n",
    "                else:\n",
    "                    output_file.write(\"\\n\")\n",
    "            \n",
    "            print(f\"{filename}\")\n",
    "\n",
    "\n",
    "start_date = datetime(2020, 6, 1).date()  # Date de début (inclusive)\n",
    "end_date = datetime(2021, 6, 30).date()   # Date de fin (exclusive)\n",
    "\n",
    "for source in relevant_sources:\n",
    "    print(\"===================\")\n",
    "    print(f\"{source}\")\n",
    "    print(\"===================\")\n",
    "        \n",
    "    # Extraction des titres des fichiers JSON\n",
    "    json_dir = f\"./data/non_covid/{source}\"\n",
    "    tmp_dir = f\"./data/tmp/{source}/spacy/non_covid/texts\"\n",
    "    extract_texts(json_dir, tmp_dir, start_date, end_date)\n",
    "    print(\"Extraction OK\")\n",
    "\n",
    "    # Prétraitement des textes avec Spacy\n",
    "    clean_dir = f\"./data/tmp/{source}/spacy/non_covid/texts_clean\"\n",
    "    preprocess_texts(tmp_dir, clean_dir)\n",
    "    print(\"Preprocessing OK\")\n",
    "    \n",
    "    # Chargement du dictionnaire d'émotions lemmatisé avec Spacy\n",
    "    emotion_dict_path = \"./lexique/emotion_dictionary_final.csv\"\n",
    "    emotion_dict_final, regex_patterns = load_dictionary(emotion_dict_path)\n",
    "\n",
    "    # Traitement des textes pour obtenir les vecteurs d'émotion\n",
    "    emotions_dir = f\"./data/tmp/{source}/spacy/non_covid/texts_emotions_len\"\n",
    "    process_texts(clean_dir, emotions_dir, emotion_dict_final, regex_patterns)\n",
    "    print(\"Sentiment Analysis OK\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5580a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
